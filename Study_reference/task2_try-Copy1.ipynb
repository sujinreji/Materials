{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import sklearn  \n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import njit, prange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset \n",
    "dataset = pd.read_csv(\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>6</th>\n",
       "      <th>148</th>\n",
       "      <th>72</th>\n",
       "      <th>35</th>\n",
       "      <th>0</th>\n",
       "      <th>33.6</th>\n",
       "      <th>0.627</th>\n",
       "      <th>50</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>116</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.201</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   6  148  72  35    0  33.6  0.627  50  1\n",
       "0  1   85  66  29    0  26.6  0.351  31  0\n",
       "1  8  183  64   0    0  23.3  0.672  32  1\n",
       "2  1   89  66  23   94  28.1  0.167  21  0\n",
       "3  0  137  40  35  168  43.1  2.288  33  1\n",
       "4  5  116  74   0    0  25.6  0.201  30  0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X Y split of the data\n",
    "X = dataset.iloc[:,0:8].values        #.value convert the dataset type to numpy\n",
    "Y = dataset.iloc[:,8].values\n",
    "\n",
    "#converting X Y into tensors\n",
    "X = torch.tensor(X, dtype= torch.float32)\n",
    "Y = torch.tensor(Y, dtype= torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([767, 8]) torch.Size([767])\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Test train split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([613, 8]) torch.Size([154, 8]) torch.Size([613]) torch.Size([154]) <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape , X_test.shape, Y_train.shape, Y_test.shape, type(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model definition\n",
    "class NN_model(nn.Module):\n",
    "    def __init__(self, input_features=8, hidden_layer1 = 20, hidden_layer2 = 20, output_features=1):\n",
    "        super().__init__()\n",
    "        self.from_input_to_H1 = nn.Linear(input_features, hidden_layer1)\n",
    "        self.from_H1_to_H2 = nn.Linear(hidden_layer1,hidden_layer2)\n",
    "        self.from_H2_to_output = nn.Linear(hidden_layer2,output_features)\n",
    "        #self.relu = nn.ReLU()\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.from_input_to_H1(x))\n",
    "        x = F.relu(self.from_H1_to_H2(x))\n",
    "        x = F.sigmoid(self.from_H2_to_output(x))\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc9e10317f0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(20)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model creating\n",
    "model1 = NN_model()\n",
    "loss_fn1 = nn.BCELoss()\n",
    "optimizer1 = optim.Adam(model1.parameters(),lr=0.0001)\n",
    "model2= NN_model()\n",
    "loss_fn2 = nn.BCELoss()\n",
    "optimizer2 = optim.Adam(model2.parameters(),lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_fn(X_train, Y_train, model, optimizer, loss_fn):\n",
    "    epoch = 100000\n",
    "    batch_size = 10\n",
    "    loss_calc=[]\n",
    "    for i in range (epoch):\n",
    "        epoch_loss = 0\n",
    "        for a in range(0, len(X_train), batch_size):\n",
    "            X_batch = X_train[a:a+batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            Y_pred = model(X_batch)\n",
    "            Y_batch = Y_train[a:a+batch_size]\n",
    "            loss = loss_fn(Y_pred.squeeze(), Y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # print(loss)\n",
    "            epoch_loss = epoch_loss + loss.item()\n",
    "            \n",
    "        epoch_loss = epoch_loss/(len(X_train)/batch_size)\n",
    "        loss_calc.append(epoch_loss)\n",
    "\n",
    "        if i % 100 ==0:\n",
    "            print(\"Epoch number\",i)\n",
    "            print(epoch_loss)\n",
    "\n",
    "    plt.plot(loss_calc)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 0\n",
      "0.8763702021726387\n",
      "Epoch number 100\n",
      "0.5826957511668494\n",
      "Epoch number 200\n",
      "0.5531472385804767\n",
      "Epoch number 300\n",
      "0.5342008209928603\n",
      "Epoch number 400\n",
      "0.5200832805081255\n",
      "Epoch number 500\n",
      "0.5071725104411313\n",
      "Epoch number 600\n",
      "0.4951756441476489\n",
      "Epoch number 700\n",
      "0.484770184434452\n",
      "Epoch number 800\n",
      "0.4753856166443662\n",
      "Epoch number 900\n",
      "0.4664872778764946\n",
      "Epoch number 1000\n",
      "0.4586134583584442\n",
      "Epoch number 1100\n",
      "0.4515002412663782\n",
      "Epoch number 1200\n",
      "0.4448052944211353\n",
      "Epoch number 1300\n",
      "0.438544477714411\n",
      "Epoch number 1400\n",
      "0.4326547864599679\n",
      "Epoch number 1500\n",
      "0.4270417667427032\n",
      "Epoch number 1600\n",
      "0.42228495437791647\n",
      "Epoch number 1700\n",
      "0.4178293319160748\n",
      "Epoch number 1800\n",
      "0.4133506323443346\n",
      "Epoch number 1900\n",
      "0.4086164245305987\n",
      "Epoch number 2000\n",
      "0.4042108525897317\n",
      "Epoch number 2100\n",
      "0.4008377875774175\n",
      "Epoch number 2200\n",
      "0.3972539773627168\n",
      "Epoch number 2300\n",
      "0.3944763335814095\n",
      "Epoch number 2400\n",
      "0.3912809860647989\n",
      "Epoch number 2500\n",
      "0.3885260354393264\n",
      "Epoch number 2600\n",
      "0.3860325655530755\n",
      "Epoch number 2700\n",
      "0.383266372572734\n",
      "Epoch number 2800\n",
      "0.38094208811274555\n",
      "Epoch number 2900\n",
      "0.37866405413454163\n",
      "Epoch number 3000\n",
      "0.37727104363200326\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_fn1\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[37], line 14\u001b[0m, in \u001b[0;36mmodel_fn\u001b[0;34m(X_train, Y_train, model, optimizer, loss_fn)\u001b[0m\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(Y_pred\u001b[38;5;241m.\u001b[39msqueeze(), Y_batch)\n\u001b[1;32m     13\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 14\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# print(loss)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m epoch_loss \u001b[38;5;241m+\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Exercises/Study_reference/pytorch/lib/python3.8/site-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Exercises/Study_reference/pytorch/lib/python3.8/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/Exercises/Study_reference/pytorch/lib/python3.8/site-packages/torch/optim/adam.py:168\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    157\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    159\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    160\u001b[0m         group,\n\u001b[1;32m    161\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    166\u001b[0m         state_steps)\n\u001b[0;32m--> 168\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/Exercises/Study_reference/pytorch/lib/python3.8/site-packages/torch/optim/adam.py:318\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 318\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Exercises/Study_reference/pytorch/lib/python3.8/site-packages/torch/optim/adam.py:394\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    393\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m--> 394\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[1;32m    397\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_fn(X_train,Y_train, model1, optimizer1,loss_fn1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN_model()\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.0001)\n",
    "# def apply_model(x):\n",
    "#     return model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from torch.func import vmap\n",
    "epoch = 100000\n",
    "batch_size = 10\n",
    "loss_calc=[]\n",
    "for i in range (epoch):\n",
    "    epoch_loss = 0\n",
    "    for a in range(0, len(X_train), batch_size):\n",
    "        X_batch = X_train[a:a+batch_size]\n",
    "        optimizer.zero_grad()\n",
    "        Y_pred = vmap(apply_model)(X_batch)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7687],\n",
       "        [0.9985],\n",
       "        [0.6726],\n",
       "        [0.9684],\n",
       "        [0.5449],\n",
       "        [0.6146],\n",
       "        [0.6787],\n",
       "        [0.9873],\n",
       "        [0.9952],\n",
       "        [0.9862]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.32 s, sys: 190 ms, total: 9.51 s\n",
      "Wall time: 9.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from torch.func import vmap\n",
    "epoch = 100000\n",
    "batch_size = 10\n",
    "loss_calc=[]\n",
    "for i in prange (epoch):\n",
    "    epoch_loss = 0\n",
    "    for a in prange(0, len(X_train), batch_size):\n",
    "        X_batch = X_train[a:a+batch_size]\n",
    "        optimizer.zero_grad()\n",
    "        Y_pred = model(X_batch)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import vmap\n",
    "from numba import prange\n",
    "\n",
    "def model_fn(X_train, Y_train, model, optimizer, loss_fn):\n",
    "    epoch = 1000\n",
    "    batch_size = 159\n",
    "    loss_calc = []\n",
    "\n",
    "    # Define a function that applies the model and calculates the loss for a single batch\n",
    "    def compute_loss(X_batch, Y_batch):\n",
    "        Y_pred = model(X_batch)\n",
    "        loss = loss_fn(Y_pred.squeeze(), Y_batch)\n",
    "        return loss\n",
    "\n",
    "    for i in prange(epoch):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        # Iterate through the dataset in batches\n",
    "        for a in prange(0, len(X_train), batch_size):\n",
    "            X_batch = X_train[a:a+batch_size]\n",
    "            Y_batch = Y_train[a:a+batch_size]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Use vmap to apply the model and compute the loss over the batch\n",
    "            losses = vmap(compute_loss)(X_batch, Y_batch)\n",
    "\n",
    "            # Sum the losses across the batch and backpropagate\n",
    "            loss = losses.sum()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        epoch_loss = epoch_loss / (len(X_train) / batch_size)\n",
    "        loss_calc.append(epoch_loss)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(\"Epoch number\", i)\n",
    "            print(epoch_loss)\n",
    "\n",
    "    return loss_calc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 0\n",
      "67.13478820825674\n",
      "Epoch number 100\n",
      "65.7927101222376\n",
      "Epoch number 200\n",
      "64.95917906146649\n",
      "Epoch number 300\n",
      "64.21829972259373\n",
      "Epoch number 400\n",
      "63.53047009665659\n",
      "Epoch number 500\n",
      "62.91081402702394\n",
      "Epoch number 600\n",
      "62.35267247111335\n",
      "Epoch number 700\n",
      "61.84785767597242\n",
      "Epoch number 800\n",
      "61.366534558244865\n",
      "Epoch number 900\n",
      "60.92561490928563\n",
      "CPU times: user 12.8 s, sys: 37.9 ms, total: 12.9 s\n",
      "Wall time: 3.22 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[67.13478820825674,\n",
       " 67.08692819492657,\n",
       " 67.07257216984063,\n",
       " 67.02460034792031,\n",
       " 66.98364773076666,\n",
       " 66.9856553381069,\n",
       " 66.95428659709788,\n",
       " 66.9084114336073,\n",
       " 66.9070598359598,\n",
       " 66.87854567709795,\n",
       " 66.85910879262704,\n",
       " 66.85130791718575,\n",
       " 66.8461993530174,\n",
       " 66.8217953966648,\n",
       " 66.78437316048203,\n",
       " 66.77406401268615,\n",
       " 66.7571542002443,\n",
       " 66.7614365681933,\n",
       " 66.73751645609372,\n",
       " 66.70712430869969,\n",
       " 66.6996647957682,\n",
       " 66.69316802406,\n",
       " 66.6805791683726,\n",
       " 66.66213668773459,\n",
       " 66.62864259405588,\n",
       " 66.62698624379095,\n",
       " 66.61482384387665,\n",
       " 66.59221278279291,\n",
       " 66.5856586226048,\n",
       " 66.56764754502264,\n",
       " 66.54958996298262,\n",
       " 66.54285869007204,\n",
       " 66.52611411687407,\n",
       " 66.50943979509114,\n",
       " 66.50640018456912,\n",
       " 66.50132724083656,\n",
       " 66.48298172694047,\n",
       " 66.46730972464283,\n",
       " 66.46759270921618,\n",
       " 66.43913890720387,\n",
       " 66.42386367698283,\n",
       " 66.42012254176677,\n",
       " 66.39642604684752,\n",
       " 66.40587337798824,\n",
       " 66.38553114077396,\n",
       " 66.38470989183735,\n",
       " 66.35504400555112,\n",
       " 66.35478278902188,\n",
       " 66.35287313788011,\n",
       " 66.33726940808444,\n",
       " 66.3067684204699,\n",
       " 66.3063083231741,\n",
       " 66.2815926880082,\n",
       " 66.2640407182653,\n",
       " 66.2668487959546,\n",
       " 66.25897766909826,\n",
       " 66.24341648593423,\n",
       " 66.22682527850154,\n",
       " 66.21534560360683,\n",
       " 66.21436703107875,\n",
       " 66.18829089315642,\n",
       " 66.18271827386604,\n",
       " 66.16938930467914,\n",
       " 66.15737730216047,\n",
       " 66.14602031987799,\n",
       " 66.13524019076424,\n",
       " 66.12632716616059,\n",
       " 66.11544413356563,\n",
       " 66.11502856181458,\n",
       " 66.08290783382746,\n",
       " 66.09544622723081,\n",
       " 66.09059492176547,\n",
       " 66.05855038193272,\n",
       " 66.0619798383355,\n",
       " 66.04353241041473,\n",
       " 66.04310595464162,\n",
       " 66.03869792571076,\n",
       " 66.01511818666442,\n",
       " 66.0023235240146,\n",
       " 65.99089629031705,\n",
       " 65.98458949426649,\n",
       " 65.97493932453298,\n",
       " 65.95124579798339,\n",
       " 65.9570479711934,\n",
       " 65.95151987744778,\n",
       " 65.92669738097447,\n",
       " 65.92375968447712,\n",
       " 65.91043368365986,\n",
       " 65.89668617435225,\n",
       " 65.8977785343836,\n",
       " 65.89149053800747,\n",
       " 65.86521057203582,\n",
       " 65.86957803324813,\n",
       " 65.86164951791189,\n",
       " 65.84307247118305,\n",
       " 65.84085806742384,\n",
       " 65.81884859592257,\n",
       " 65.82132421621102,\n",
       " 65.81394188089044,\n",
       " 65.78385151774421,\n",
       " 65.7927101222376,\n",
       " 65.78482316407644,\n",
       " 65.77342264570575,\n",
       " 65.77090249987256,\n",
       " 65.74740983300264,\n",
       " 65.75487429321687,\n",
       " 65.75010016536247,\n",
       " 65.73106994753367,\n",
       " 65.7151664124033,\n",
       " 65.71440552031625,\n",
       " 65.68917833611394,\n",
       " 65.69080599213892,\n",
       " 65.678818726034,\n",
       " 65.66258669132898,\n",
       " 65.6597944449445,\n",
       " 65.65269509419726,\n",
       " 65.64034568699499,\n",
       " 65.63301975069777,\n",
       " 65.62613313310882,\n",
       " 65.61609509640768,\n",
       " 65.60555045795286,\n",
       " 65.60149467555384,\n",
       " 65.5779129575944,\n",
       " 65.58645889381795,\n",
       " 65.57198017611978,\n",
       " 65.5583791063816,\n",
       " 65.56370535099099,\n",
       " 65.54594757428566,\n",
       " 65.5329807461961,\n",
       " 65.53364269262815,\n",
       " 65.53105031646679,\n",
       " 65.51992882484325,\n",
       " 65.49645099982158,\n",
       " 65.50618032607908,\n",
       " 65.48698783737606,\n",
       " 65.473606426992,\n",
       " 65.47153351551947,\n",
       " 65.46671980940305,\n",
       " 65.44075943789709,\n",
       " 65.45247559290726,\n",
       " 65.4487384155174,\n",
       " 65.41276177535438,\n",
       " 65.42557820604831,\n",
       " 65.42801424807476,\n",
       " 65.40701600116773,\n",
       " 65.40484513349674,\n",
       " 65.40656283006777,\n",
       " 65.36956507074503,\n",
       " 65.37929241808943,\n",
       " 65.37717003378938,\n",
       " 65.34645136573381,\n",
       " 65.3496740257176,\n",
       " 65.34842434209479,\n",
       " 65.33045680060084,\n",
       " 65.30711848895274,\n",
       " 65.3154833546276,\n",
       " 65.31373795327316,\n",
       " 65.2881229021032,\n",
       " 65.29912368002763,\n",
       " 65.2942367541265,\n",
       " 65.2675966259706,\n",
       " 65.26246332538848,\n",
       " 65.26581263581069,\n",
       " 65.25565685378008,\n",
       " 65.234243035122,\n",
       " 65.22805893168365,\n",
       " 65.22954608487849,\n",
       " 65.2061226799671,\n",
       " 65.20561706766998,\n",
       " 65.20011469979465,\n",
       " 65.18724780681667,\n",
       " 65.17917285191099,\n",
       " 65.18107854522655,\n",
       " 65.16557277162934,\n",
       " 65.14216420856619,\n",
       " 65.15421479989033,\n",
       " 65.14494458147207,\n",
       " 65.11661644044163,\n",
       " 65.138993000323,\n",
       " 65.12149248232072,\n",
       " 65.0940785991426,\n",
       " 65.11377768959929,\n",
       " 65.10598472981042,\n",
       " 65.08432354701479,\n",
       " 65.0720641803586,\n",
       " 65.06979535648912,\n",
       " 65.06396053121297,\n",
       " 65.04244776689987,\n",
       " 65.04873180544979,\n",
       " 65.04394877248643,\n",
       " 65.01848510816863,\n",
       " 65.01498638980735,\n",
       " 65.00248361683981,\n",
       " 64.99177373914112,\n",
       " 64.99140269293483,\n",
       " 64.98035837892219,\n",
       " 64.97679336697203,\n",
       " 64.96200198100404,\n",
       " 64.94699291459499,\n",
       " 64.94443121158669,\n",
       " 64.95917906146649,\n",
       " 64.92713650054684,\n",
       " 64.92405236448005,\n",
       " 64.91759319212075,\n",
       " 64.91185038630375,\n",
       " 64.90127507469589,\n",
       " 64.89147945484952,\n",
       " 64.8830245486286,\n",
       " 64.86681725033733,\n",
       " 64.87242054278077,\n",
       " 64.85673567754799,\n",
       " 64.85652888112901,\n",
       " 64.85443618052547,\n",
       " 64.8410112340532,\n",
       " 64.82418453596156,\n",
       " 64.8176036604466,\n",
       " 64.80778132527338,\n",
       " 64.81101387982268,\n",
       " 64.79555263177022,\n",
       " 64.78099673646115,\n",
       " 64.77425754789816,\n",
       " 64.77404580419643,\n",
       " 64.76687818094723,\n",
       " 64.76229204983736,\n",
       " 64.74521798760817,\n",
       " 64.74563850664198,\n",
       " 64.74556231848763,\n",
       " 64.73176632580905,\n",
       " 64.7115982329476,\n",
       " 64.71664149298365,\n",
       " 64.73164462265339,\n",
       " 64.69828212669584,\n",
       " 64.68868142978985,\n",
       " 64.68953632024918,\n",
       " 64.67908073288388,\n",
       " 64.66463862507807,\n",
       " 64.65746308617646,\n",
       " 64.65474307012013,\n",
       " 64.64397976176774,\n",
       " 64.6336617088629,\n",
       " 64.63028964094003,\n",
       " 64.62499703785335,\n",
       " 64.61125843365468,\n",
       " 64.60529497902701,\n",
       " 64.60712745255786,\n",
       " 64.60257793134033,\n",
       " 64.58696925426192,\n",
       " 64.57844904390868,\n",
       " 64.57772080388777,\n",
       " 64.56449770655173,\n",
       " 64.5592773337932,\n",
       " 64.55034550951511,\n",
       " 64.53689384771599,\n",
       " 64.53465569699958,\n",
       " 64.52985089599211,\n",
       " 64.51894015861374,\n",
       " 64.51542461949111,\n",
       " 64.50579028106242,\n",
       " 64.49565131979315,\n",
       " 64.49360710256057,\n",
       " 64.48264788181125,\n",
       " 64.47603831205633,\n",
       " 64.47671707924972,\n",
       " 64.46557579849517,\n",
       " 64.45603347952564,\n",
       " 64.4575344851122,\n",
       " 64.44430940886306,\n",
       " 64.44120350475218,\n",
       " 64.42843456797266,\n",
       " 64.42237810442927,\n",
       " 64.42170824234482,\n",
       " 64.41284469056869,\n",
       " 64.39331776555171,\n",
       " 64.39400444839751,\n",
       " 64.40494684838549,\n",
       " 64.38462143193256,\n",
       " 64.36272179010837,\n",
       " 64.38527942053841,\n",
       " 64.37749437640193,\n",
       " 64.35675734602414,\n",
       " 64.34671238312714,\n",
       " 64.34720711140221,\n",
       " 64.33982774445128,\n",
       " 64.33165681226025,\n",
       " 64.31919163864161,\n",
       " 64.3036858650444,\n",
       " 64.30595765728351,\n",
       " 64.30400051222735,\n",
       " 64.292863189299,\n",
       " 64.28221762627606,\n",
       " 64.27864173030387,\n",
       " 64.27254469904192,\n",
       " 64.274579021709,\n",
       " 64.25303756315596,\n",
       " 64.24175577957129,\n",
       " 64.2455038409832,\n",
       " 64.24224852893326,\n",
       " 64.2309786188272,\n",
       " 64.21792768693088,\n",
       " 64.22039440211037,\n",
       " 64.21829972259373,\n",
       " 64.20942528679554,\n",
       " 64.18731588018271,\n",
       " 64.20168575766036,\n",
       " 64.19499505246833,\n",
       " 64.18281978961889,\n",
       " 64.16361641689382,\n",
       " 64.17699584836478,\n",
       " 64.16998752762015,\n",
       " 64.1523633275491,\n",
       " 64.13245051447758,\n",
       " 64.14124579375175,\n",
       " 64.14142389593079,\n",
       " 64.1297275300516,\n",
       " 64.10903898304477,\n",
       " 64.1150192584338,\n",
       " 64.10840177302649,\n",
       " 64.09126537503465,\n",
       " 64.07299505983636,\n",
       " 64.08413436167781,\n",
       " 64.07871115032651,\n",
       " 64.06996930170604,\n",
       " 64.05537976487426,\n",
       " 64.07021072910428,\n",
       " 64.05633261153204,\n",
       " 64.0381088007916,\n",
       " 64.034485410905,\n",
       " 64.03113313211314,\n",
       " 64.0232362793865,\n",
       " 64.01108278458115,\n",
       " 64.00625324715993,\n",
       " 64.00842510428748,\n",
       " 64.00109125233786,\n",
       " 63.99475378313423,\n",
       " 63.98779988249986,\n",
       " 63.96624852938131,\n",
       " 63.98396573836808,\n",
       " 63.96835409292,\n",
       " 63.95556635646603,\n",
       " 63.94019514895964,\n",
       " 63.94367308873337,\n",
       " 63.93730296746359,\n",
       " 63.91896536921988,\n",
       " 63.90272146103627,\n",
       " 63.90515849251926,\n",
       " 63.89833816851917,\n",
       " 63.888431729539185,\n",
       " 63.88187855880763,\n",
       " 63.87331085453998,\n",
       " 63.8644759970038,\n",
       " 63.856155656873696,\n",
       " 63.854565600197624,\n",
       " 63.850537522582016,\n",
       " 63.8371195023056,\n",
       " 63.83018044351948,\n",
       " 63.82576845676242,\n",
       " 63.8195556590841,\n",
       " 63.808110615168665,\n",
       " 63.8090941349795,\n",
       " 63.79367048627596,\n",
       " 63.785643025284685,\n",
       " 63.78268949748253,\n",
       " 63.77904137118217,\n",
       " 63.771868800650225,\n",
       " 63.758323140478836,\n",
       " 63.75979347291234,\n",
       " 63.75080821798054,\n",
       " 63.74038132485519,\n",
       " 63.73499571285279,\n",
       " 63.73533014916674,\n",
       " 63.72375746535633,\n",
       " 63.71664920950014,\n",
       " 63.71362740919602,\n",
       " 63.706767506933915,\n",
       " 63.69571824563853,\n",
       " 63.68684183092724,\n",
       " 63.68078734629695,\n",
       " 63.68332530234805,\n",
       " 63.663958669292214,\n",
       " 63.65617659352538,\n",
       " 63.65389985400351,\n",
       " 63.652760989514306,\n",
       " 63.65282332527696,\n",
       " 63.63039432419844,\n",
       " 63.63636371556542,\n",
       " 63.63040421876394,\n",
       " 63.6175462308949,\n",
       " 63.61527147028614,\n",
       " 63.604727821287874,\n",
       " 63.59702490204505,\n",
       " 63.59201627298825,\n",
       " 63.58195943661266,\n",
       " 63.57650456265174,\n",
       " 63.57640165917053,\n",
       " 63.570123557359906,\n",
       " 63.55679359871645,\n",
       " 63.55127243116669,\n",
       " 63.54941324230898,\n",
       " 63.535442115821034,\n",
       " 63.530709545141725,\n",
       " 63.53047009665659,\n",
       " 63.52028660984257,\n",
       " 63.50964599410239,\n",
       " 63.509070130390214,\n",
       " 63.50090018765573,\n",
       " 63.49862146922076,\n",
       " 63.488176765877505,\n",
       " 63.476290424340704,\n",
       " 63.47730164893495,\n",
       " 63.472341503249105,\n",
       " 63.45725229085951,\n",
       " 63.46066096867473,\n",
       " 63.4504181144677,\n",
       " 63.44636628989489,\n",
       " 63.4428596558812,\n",
       " 63.42731232510889,\n",
       " 63.42364836750373,\n",
       " 63.41857443431463,\n",
       " 63.40635959320318,\n",
       " 63.4025848164644,\n",
       " 63.3942763498129,\n",
       " 63.39161372223648,\n",
       " 63.38429768050476,\n",
       " 63.37872011393163,\n",
       " 63.3730732854,\n",
       " 63.36453724374196,\n",
       " 63.359915492196265,\n",
       " 63.34889789351048,\n",
       " 63.344723376325454,\n",
       " 63.34928477102159,\n",
       " 63.332165193791106,\n",
       " 63.32737523463189,\n",
       " 63.32260506460368,\n",
       " 63.31511685743223,\n",
       " 63.308870418231216,\n",
       " 63.30380835852071,\n",
       " 63.299672430141136,\n",
       " 63.29640425515603,\n",
       " 63.28750013566134,\n",
       " 63.28016331534207,\n",
       " 63.27491226943049,\n",
       " 63.2685401692476,\n",
       " 63.26264597657843,\n",
       " 63.26192268384028,\n",
       " 63.24699475286835,\n",
       " 63.24027139561017,\n",
       " 63.237217932696446,\n",
       " 63.22840385374782,\n",
       " 63.226137008791454,\n",
       " 63.22006471394326,\n",
       " 63.21309696091719,\n",
       " 63.210419491492516,\n",
       " 63.20988023767269,\n",
       " 63.19546682410684,\n",
       " 63.185491123168354,\n",
       " 63.18879887641546,\n",
       " 63.1786480416676,\n",
       " 63.16617000511381,\n",
       " 63.164109956576425,\n",
       " 63.16307300611188,\n",
       " 63.15005670519482,\n",
       " 63.1502793329186,\n",
       " 63.14217766268607,\n",
       " 63.13484480019301,\n",
       " 63.129359253079045,\n",
       " 63.122705157779365,\n",
       " 63.113705060999315,\n",
       " 63.107181573964255,\n",
       " 63.104341833665366,\n",
       " 63.095378346777665,\n",
       " 63.09223088549168,\n",
       " 63.091687673845655,\n",
       " 63.081364673658065,\n",
       " 63.07715354658068,\n",
       " 63.07304136515831,\n",
       " 63.06341395292547,\n",
       " 63.054910563333586,\n",
       " 63.04897778185896,\n",
       " 63.04002814736297,\n",
       " 63.03340769358599,\n",
       " 63.03696478988374,\n",
       " 63.030892495035545,\n",
       " 63.021449121721034,\n",
       " 63.01368584562865,\n",
       " 63.01712123877073,\n",
       " 63.01578151460184,\n",
       " 63.00124639788032,\n",
       " 62.981631411230396,\n",
       " 62.98916414394659,\n",
       " 62.978964825827774,\n",
       " 62.97030411264441,\n",
       " 62.96594753545416,\n",
       " 62.95877397546566,\n",
       " 62.94986194031857,\n",
       " 62.94684409784065,\n",
       " 62.94639785293654,\n",
       " 62.9360817789448,\n",
       " 62.931626256099534,\n",
       " 62.92799989784328,\n",
       " 62.920384050776875,\n",
       " 62.91081402702394,\n",
       " 62.90986118036616,\n",
       " 62.903974903349386,\n",
       " 62.897917450349446,\n",
       " 62.88524844868149,\n",
       " 62.88480616160357,\n",
       " 62.88543545596946,\n",
       " 62.86830697363003,\n",
       " 62.86405626829064,\n",
       " 62.86375745241249,\n",
       " 62.85604167023452,\n",
       " 62.8461401785373,\n",
       " 62.83964340682909,\n",
       " 62.834931614737336,\n",
       " 62.823729977133226,\n",
       " 62.82772936050888,\n",
       " 62.81855511937601,\n",
       " 62.81396799880957,\n",
       " 62.80796991320263,\n",
       " 62.80349658013946,\n",
       " 62.7926333366755,\n",
       " 62.794449978901554,\n",
       " 62.79618053840775,\n",
       " 62.77455200767829,\n",
       " 62.76965716612475,\n",
       " 62.76910010208703,\n",
       " 62.758661335483076,\n",
       " 62.75294722390603,\n",
       " 62.74367304766159,\n",
       " 62.74181781663008,\n",
       " 62.752816615641414,\n",
       " 62.734286073370434,\n",
       " 62.71414568529238,\n",
       " 62.727046219793074,\n",
       " 62.72354453306214,\n",
       " 62.71263973242307,\n",
       " 62.70010430738938,\n",
       " 62.70438469642527,\n",
       " 62.70318646454306,\n",
       " 62.685890764046654,\n",
       " 62.67716276781789,\n",
       " 62.67856383829288,\n",
       " 62.680113327250396,\n",
       " 62.666732906322885,\n",
       " 62.66154023834776,\n",
       " 62.65878064402943,\n",
       " 62.660181714504425,\n",
       " 62.64359545435447,\n",
       " 62.634348982893435,\n",
       " 62.63676325687577,\n",
       " 62.632586760777635,\n",
       " 62.61665749977697,\n",
       " 62.61061785699493,\n",
       " 62.616137045631596,\n",
       " 62.60971448316465,\n",
       " 62.59976351863992,\n",
       " 62.58333557153799,\n",
       " 62.580828288639935,\n",
       " 62.58620598498994,\n",
       " 62.577455231260515,\n",
       " 62.568295831975895,\n",
       " 62.55916215856157,\n",
       " 62.56838884089161,\n",
       " 62.570853577158,\n",
       " 62.55483130724162,\n",
       " 62.53341847804009,\n",
       " 62.532725858454995,\n",
       " 62.541910983609924,\n",
       " 62.52267792718835,\n",
       " 62.5090343108185,\n",
       " 62.517779127808616,\n",
       " 62.520403166579584,\n",
       " 62.50637860943793,\n",
       " 62.4939728032123,\n",
       " 62.48868613686492,\n",
       " 62.494002486908805,\n",
       " 62.49517301400762,\n",
       " 62.48110591023431,\n",
       " 62.47081457265633,\n",
       " 62.459605019399824,\n",
       " 62.47072651102337,\n",
       " 62.47172190431281,\n",
       " 62.45809115087811,\n",
       " 62.44240727510188,\n",
       " 62.4375213386573,\n",
       " 62.44187593693446,\n",
       " 62.438030908780625,\n",
       " 62.427772223268796,\n",
       " 62.41073971821473,\n",
       " 62.40591512807625,\n",
       " 62.408356117385445,\n",
       " 62.41391587374067,\n",
       " 62.39563665343343,\n",
       " 62.382278000550016,\n",
       " 62.382569890232304,\n",
       " 62.38832951681066,\n",
       " 62.37582575438657,\n",
       " 62.368537417438255,\n",
       " 62.36344963185745,\n",
       " 62.35744066222846,\n",
       " 62.35267247111335,\n",
       " 62.355339056515966,\n",
       " 62.35103887834907,\n",
       " 62.33813141765253,\n",
       " 62.32966562740955,\n",
       " 62.326620080148224,\n",
       " 62.32941826327201,\n",
       " 62.31775158108934,\n",
       " 62.31320601769801,\n",
       " 62.30269403130934,\n",
       " 62.30813307396545,\n",
       " 62.30812120048685,\n",
       " 62.294837746301255,\n",
       " 62.28465920676999,\n",
       " 62.27626069957242,\n",
       " 62.27547507107161,\n",
       " 62.278954000301894,\n",
       " 62.265363814585754,\n",
       " 62.252930303576726,\n",
       " 62.24749126092062,\n",
       " 62.25402662143428,\n",
       " 62.253286507934774,\n",
       " 62.237836143904374,\n",
       " 62.23194491960486,\n",
       " 62.227224222404146,\n",
       " 62.222127531714385,\n",
       " 62.225609429314325,\n",
       " 62.22541351691739,\n",
       " 62.201830809501416,\n",
       " 62.203120071386245,\n",
       " 62.21021447485073,\n",
       " 62.205040606550064,\n",
       " 62.19862200190932,\n",
       " 62.177885960988085,\n",
       " 62.17235588832936,\n",
       " 62.16370111188531,\n",
       " 62.17338393368496,\n",
       " 62.180761321722784,\n",
       " 62.15960476176774,\n",
       " 62.14839718742433,\n",
       " 62.14914224820659,\n",
       " 62.145070634502765,\n",
       " 62.151708898497645,\n",
       " 62.13913983194124,\n",
       " 62.119473393550706,\n",
       " 62.10986478099232,\n",
       " 62.1186976596154,\n",
       " 62.11819303677483,\n",
       " 62.10526578694728,\n",
       " 62.097522299985904,\n",
       " 62.09758463574856,\n",
       " 62.08580713443227,\n",
       " 62.09376533346503,\n",
       " 62.07966458816902,\n",
       " 62.079944604372706,\n",
       " 62.07656956808019,\n",
       " 62.07434526975548,\n",
       " 62.06613970658519,\n",
       " 62.05046275700481,\n",
       " 62.047496366267495,\n",
       " 62.051213754526366,\n",
       " 62.03907015428652,\n",
       " 62.03726142771287,\n",
       " 62.02960996021066,\n",
       " 62.022723342621696,\n",
       " 62.018792231748,\n",
       " 62.018573561850424,\n",
       " 62.01967482699072,\n",
       " 62.0177008611732,\n",
       " 62.005935233335514,\n",
       " 61.98646371788536,\n",
       " 61.98629551027183,\n",
       " 61.99614950805465,\n",
       " 61.992501381754295,\n",
       " 61.98975168200146,\n",
       " 61.9655654060899,\n",
       " 61.97324952565827,\n",
       " 61.97141903104051,\n",
       " 61.96228634708274,\n",
       " 61.94782346068938,\n",
       " 61.94117431267245,\n",
       " 61.94404670503751,\n",
       " 61.94541413398979,\n",
       " 61.93067914704515,\n",
       " 61.922008539296286,\n",
       " 61.917368977532696,\n",
       " 61.90498790772082,\n",
       " 61.91728190535628,\n",
       " 61.92256164550781,\n",
       " 61.90763866181864,\n",
       " 61.89384464805317,\n",
       " 61.88982844391616,\n",
       " 61.890441906977244,\n",
       " 61.89220313963649,\n",
       " 61.87580586568761,\n",
       " 61.87171743122244,\n",
       " 61.8671402052215,\n",
       " 61.86690174619292,\n",
       " 61.86793869665747,\n",
       " 61.85861603704207,\n",
       " 61.84785767597242,\n",
       " 61.840209176839856,\n",
       " 61.83548452181295,\n",
       " 61.833574870671185,\n",
       " 61.83085287570176,\n",
       " 61.83012463568085,\n",
       " 61.8333205803378,\n",
       " 61.807903420477864,\n",
       " 61.80536051714401,\n",
       " 61.81307234149578,\n",
       " 61.80936187943277,\n",
       " 61.795109747284585,\n",
       " 61.78679534389378,\n",
       " 61.781290997105366,\n",
       " 61.78271086725481,\n",
       " 61.77594694227807,\n",
       " 61.776102286956444,\n",
       " 61.76303948158152,\n",
       " 61.75367427533447,\n",
       " 61.75928449397375,\n",
       " 61.767422774098634,\n",
       " 61.75554236930113,\n",
       " 61.73537823426587,\n",
       " 61.73294812897874,\n",
       " 61.72879537983781,\n",
       " 61.73479841272749,\n",
       " 61.734847885555006,\n",
       " 61.72529962984618,\n",
       " 61.70965929015815,\n",
       " 61.70090161023288,\n",
       " 61.70289932300761,\n",
       " 61.7053680171002,\n",
       " 61.6931739545763,\n",
       " 61.696968520446084,\n",
       " 61.682601611338086,\n",
       " 61.675829770708944,\n",
       " 61.67416847316126,\n",
       " 61.66818522940258,\n",
       " 61.67270308801051,\n",
       " 61.672545764419034,\n",
       " 61.65637210765048,\n",
       " 61.6510775256507,\n",
       " 61.64520411156908,\n",
       " 61.63159215780884,\n",
       " 61.64798349501842,\n",
       " 61.64173012962155,\n",
       " 61.62912148480314,\n",
       " 61.62074375619313,\n",
       " 61.61269155878811,\n",
       " 61.619201193431465,\n",
       " 61.61950792496201,\n",
       " 61.610079393495745,\n",
       " 61.58934137366141,\n",
       " 61.587787926877695,\n",
       " 61.58929684811665,\n",
       " 61.59058907837114,\n",
       " 61.58155929789458,\n",
       " 61.56165242156236,\n",
       " 61.56352348389867,\n",
       " 61.56971847135908,\n",
       " 61.568308495775135,\n",
       " 61.55327469295234,\n",
       " 61.54306646972457,\n",
       " 61.53960139288599,\n",
       " 61.54066110085119,\n",
       " 61.53326986042166,\n",
       " 61.52991263434704,\n",
       " 61.52585091520873,\n",
       " 61.522521393917515,\n",
       " 61.51278019418141,\n",
       " 61.51220037264303,\n",
       " 61.50657036487275,\n",
       " 61.50320324423263,\n",
       " 61.493410592755914,\n",
       " 61.49820055191513,\n",
       " 61.4982945502874,\n",
       " 61.48194180188327,\n",
       " 61.474206230574296,\n",
       " 61.46700892362859,\n",
       " 61.46626584175944,\n",
       " 61.46498053770081,\n",
       " 61.47033448709361,\n",
       " 61.456016061356564,\n",
       " 61.44934514529554,\n",
       " 61.448754439735104,\n",
       " 61.44255252607884,\n",
       " 61.44786096047033,\n",
       " 61.432778674276584,\n",
       " 61.42231418180233,\n",
       " 61.41441535016258,\n",
       " 61.41592130303189,\n",
       " 61.42278714203329,\n",
       " 61.406394815367165,\n",
       " 61.399721920393034,\n",
       " 61.397517411199324,\n",
       " 61.39140454863258,\n",
       " 61.39113739536404,\n",
       " 61.392039779737765,\n",
       " 61.3876080038497,\n",
       " 61.3725009812422,\n",
       " 61.366534558244865,\n",
       " 61.369659262030204,\n",
       " 61.36985121660093,\n",
       " 61.36013376382203,\n",
       " 61.352450633710205,\n",
       " 61.34736581649905,\n",
       " 61.344670536856476,\n",
       " 61.34183673329688,\n",
       " 61.34179517612178,\n",
       " 61.33924237822242,\n",
       " 61.331057593639684,\n",
       " 61.32097997867653,\n",
       " 61.31683217681835,\n",
       " 61.30521298855009,\n",
       " 61.31054516989878,\n",
       " 61.31560920852238,\n",
       " 61.309950506512145,\n",
       " 61.28789650946613,\n",
       " 61.2894667770112,\n",
       " 61.28784109989932,\n",
       " 61.28403565000749,\n",
       " 61.29076494400498,\n",
       " 61.28113159503284,\n",
       " 61.2567761220512,\n",
       " 61.26526269088173,\n",
       " 61.26629370460697,\n",
       " 61.25142514102805,\n",
       " 61.245308320635104,\n",
       " 61.242094565760254,\n",
       " 61.23643388483692,\n",
       " 61.23548400654879,\n",
       " 61.231955604491,\n",
       " 61.23004793226233,\n",
       " 61.22359271772923,\n",
       " 61.21814575942072,\n",
       " 61.206662126699825,\n",
       " 61.20460108870589,\n",
       " 61.203486960630435,\n",
       " 61.20337119421407,\n",
       " 61.193357893926674,\n",
       " 61.187539889411866,\n",
       " 61.18185051424857,\n",
       " 61.17167494308696,\n",
       " 61.176592542141144,\n",
       " 61.18264010057558,\n",
       " 61.174681901542826,\n",
       " 61.16688003664499,\n",
       " 61.15660650928491,\n",
       " 61.14632803464208,\n",
       " 61.146195447464365,\n",
       " 61.14938743429511,\n",
       " 61.13811752418904,\n",
       " 61.12683178277817,\n",
       " 61.13070946299816,\n",
       " 61.12724042833338,\n",
       " 61.122629560809735,\n",
       " 61.12788456454752,\n",
       " 61.11679968281633,\n",
       " 61.10758190559525,\n",
       " 61.10061019474298,\n",
       " 61.0946685081594,\n",
       " 61.08811533742784,\n",
       " 61.09402635085836,\n",
       " 61.08359549990681,\n",
       " 61.073810764082495,\n",
       " 61.0684350466456,\n",
       " 61.06965207820227,\n",
       " 61.06942648210884,\n",
       " 61.063023708772896,\n",
       " 61.05317663718593,\n",
       " 61.04845989781142,\n",
       " 61.0443625582373,\n",
       " 61.038961114930096,\n",
       " 61.03488356448698,\n",
       " 61.02778421373974,\n",
       " 61.02496822039805,\n",
       " 61.02611005325691,\n",
       " 61.016952632885385,\n",
       " 61.00796243067084,\n",
       " 61.00254317714573,\n",
       " 60.99601573228448,\n",
       " 60.99456716789507,\n",
       " 60.98879665729467,\n",
       " 60.99368853047855,\n",
       " 60.98476462185286,\n",
       " 60.98420953672823,\n",
       " 60.97126843450898,\n",
       " 60.972009537465034,\n",
       " 60.97564084300404,\n",
       " 60.976637225750025,\n",
       " 60.96695044612418,\n",
       " 60.95624551570824,\n",
       " 60.94142246713063,\n",
       " 60.95001589726858,\n",
       " 60.95432794891408,\n",
       " 60.95077085261633,\n",
       " 60.93195039957616,\n",
       " 60.92021049760878,\n",
       " 60.93465062650149,\n",
       " 60.93170897217793,\n",
       " 60.92561490928563,\n",
       " 60.90925721359876,\n",
       " 60.90057572182785,\n",
       " 60.89932208037882,\n",
       " 60.893226038573424,\n",
       " 60.88764748254375,\n",
       " 60.88158310334796,\n",
       " 60.87839309543031,\n",
       " 60.86815222013639,\n",
       " 60.86565977908659,\n",
       " 60.864020249583014,\n",
       " 60.858857265304394,\n",
       " 60.862405456493185,\n",
       " 60.85831009583217,\n",
       " 60.84821566010766,\n",
       " 60.83487679635526,\n",
       " 60.84029407096726,\n",
       " 60.839311540612975,\n",
       " 60.83026691828817,\n",
       " 60.82090962769352,\n",
       " 60.82340404765641,\n",
       " 60.817324826612364,\n",
       " 60.81245175310294,\n",
       " 60.810410504240004,\n",
       " 60.81228255603286,\n",
       " 60.804279831455354,\n",
       " 60.78536142221672,\n",
       " 60.78590067603655,\n",
       " 60.7862707327863,\n",
       " 60.78313316606581,\n",
       " 60.77865587517644,\n",
       " 60.77065710842513,\n",
       " 60.76563264806352,\n",
       " 60.7622101178566,\n",
       " 60.75798315947441,\n",
       " 60.75065920209029,\n",
       " 60.74622050000637,\n",
       " 60.73880749153275,\n",
       " 60.73663266603554,\n",
       " 60.73672468549471,\n",
       " 60.73211282851451,\n",
       " 60.74437912136657,\n",
       " 60.730847313586885,\n",
       " 60.7180140621316,\n",
       " 60.71560968271476,\n",
       " 60.715085270743195,\n",
       " 60.70752285433049,\n",
       " 60.69903727495651,\n",
       " 60.69346960294889,\n",
       " 60.6949933660361,\n",
       " 60.69062293645414,\n",
       " 60.68283987123076,\n",
       " 60.680741233887915,\n",
       " 60.66714610088903,\n",
       " 60.670963424259455,\n",
       " 60.672537649630726,\n",
       " 60.66622887466705,\n",
       " 60.6561769855742,\n",
       " 60.65246751296773,\n",
       " 60.651083263254094,\n",
       " 60.648736272317166,\n",
       " 60.63624636228478,\n",
       " 60.64252347463885,\n",
       " 60.63443664625458,\n",
       " 60.6209384799976,\n",
       " 60.62806058824549,\n",
       " 60.63849044974049,\n",
       " 60.62254041015227,\n",
       " 60.60361903254399,\n",
       " 60.61379163533595,\n",
       " 60.61019100295,\n",
       " 60.59931390709434,\n",
       " 60.59211857906174,\n",
       " 60.58087241591287,\n",
       " 60.58270587890028,\n",
       " 60.580733891995855,\n",
       " 60.574268782897256,\n",
       " 60.5709837871508,\n",
       " 60.565716909934416,\n",
       " 60.56026896216936,\n",
       " 60.55807038971495,\n",
       " 60.552763934236566,\n",
       " 60.54355803049408,\n",
       " 60.54075687900065,\n",
       " 60.538261469581194,\n",
       " 60.52908030225247,\n",
       " 60.5253391670364,\n",
       " 60.525958566836785,\n",
       " 60.515766174913814,\n",
       " 60.517849970408406,\n",
       " 60.51806072465359,\n",
       " 60.50716779749312,\n",
       " 60.50523044156795,\n",
       " 60.51359926506902,\n",
       " 60.49985571358761,\n",
       " 60.490303500052576,\n",
       " 60.4940337512466,\n",
       " 60.49350241307917,\n",
       " 60.485611497091824,\n",
       " 60.47349461217884]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model_fn(X_train, Y_train, model, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kernelforpytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
